{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNSpCMnMBALx",
        "outputId": "74433b4c-db8b-4d43-c8f9-8d1b0ab3d6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "iIdb52Gv_4v2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "class RetrieveTopRestaurant():\n",
        "    def get_top_restaurant(self, source_file_path:str, destination_file_path:str, num_restaurant:int):\n",
        "        # Open the original JSON file and load each line\n",
        "        with open(source_file_path, 'r') as file:\n",
        "            data = [json.loads(line) for line in file]\n",
        "\n",
        "        filtered_data=[]\n",
        "\n",
        "        counter=0\n",
        "\n",
        "        # Filter out lines with the desired tag\n",
        "        for item in data:\n",
        "            if(item['categories']==None):\n",
        "                continue\n",
        "            if(\"Restaurants\" in item['categories'] and item['city'] == \"Edmonton\" and item[\"is_open\"]==1 and \n",
        "            not pd.isna(item['attributes']) and not pd.isna(item['hours'])):\n",
        "                counter+=1\n",
        "                try:\n",
        "                    filtered_data.append(item)\n",
        "                except:\n",
        "                    print(filtered_data)\n",
        "                \n",
        "                if(counter == num_restaurant):\n",
        "                    break\n",
        "\n",
        "        # Write the filtered lines to a new JSON file\n",
        "        with open(destination_file_path, 'w') as file:\n",
        "            for item in filtered_data:\n",
        "                file.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "zVSJsw3A_tm0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "class Json_to_CSV():\n",
        "    def get_csv(self, source_file_path:str, destination_file_path:str):\n",
        "        \"\"\"This function takes in a json file as an input(source_file_path) and outputs a csv file(destination_file_path)\n",
        "\n",
        "        Args:\n",
        "            source_file_path:str : input json file path\n",
        "            destination_file_path:str : output csv file path\n",
        "        \"\"\"\n",
        "        with open(destination_file_path, \"w\", newline='') as file:\n",
        "            writer=csv.writer(file)\n",
        "            writer.writerow([\"business_id\", \"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\",\n",
        "                            \"longitude\", \"stars\", \"review_count\", \"is_open\", \"attributes\", \"categories\", \"hours\"])\n",
        "\n",
        "            with open(source_file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    # Load the JSON object from the line\n",
        "                    json_obj = json.loads(line)\n",
        "\n",
        "                    business_id=json_obj[\"business_id\"]\n",
        "                    name=json_obj[\"name\"]\n",
        "                    address=json_obj[\"address\"]\n",
        "                    city=json_obj[\"city\"]\n",
        "                    state=json_obj[\"state\"]\n",
        "                    postal_code=json_obj[\"postal_code\"]\n",
        "                    latitude=json_obj[\"latitude\"]\n",
        "                    longitude=json_obj[\"longitude\"]\n",
        "                    stars=json_obj[\"stars\"]\n",
        "                    review_count=json_obj[\"review_count\"]\n",
        "                    is_open=json_obj[\"is_open\"]\n",
        "                    attributes=json_obj[\"attributes\"]\n",
        "                    categories=json_obj[\"categories\"]\n",
        "                    hours=json_obj[\"hours\"]\n",
        "\n",
        "                    writer.writerow([business_id, name, address, city, state, postal_code, latitude, longitude,\n",
        "                                    stars, review_count, is_open, attributes, categories, hours])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "kwgaXY1chHHh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "class Remove_Weird_Stuff():\n",
        "    def clean(self, source_file_path:str):\n",
        "        \"\"\"\n",
        "        Takes in a csv file and change all the attributes values inside the file into normal values\n",
        "\n",
        "        :param source_file_path: file path to the file\n",
        "        :return: A file without the weird u'\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(source_file_path)\n",
        "\n",
        "        size = len(df[\"attributes\"])\n",
        "\n",
        "        for i in range(size):\n",
        "            for key, value in ast.literal_eval(df[\"attributes\"][i]).items():\n",
        "                if(value == \"{}\"):\n",
        "                    dic = ast.literal_eval(df[\"attributes\"][i])\n",
        "                    dic.pop(key)\n",
        "                else:\n",
        "                    value = value.replace(\"u'\", \"\")\n",
        "                    value = value.replace(\"'\", \"\")\n",
        "                    value = value.replace(\"\\\"\", \"\")\n",
        "\n",
        "                    dic = ast.literal_eval(df[\"attributes\"][i])\n",
        "                    dic[key] = value\n",
        "\n",
        "                df[\"attributes\"][i] = str(dic)\n",
        "\n",
        "        df.to_csv(source_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "hke0oSDlAb-z"
      },
      "outputs": [],
      "source": [
        "class FilterReview():\n",
        "    def filter_review(self, source_file_path:str, destination_file_path:str):\n",
        "        #Records the list of unique business_id\n",
        "        list_of_unique_business_id=[]\n",
        "\n",
        "        #Loop through the restaurant info file to collect all the business_id\n",
        "        with open(source_file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                dict=json.loads(line)\n",
        "                list_of_unique_business_id.append(dict[\"business_id\"])\n",
        "\n",
        "        with open(destination_file_path, \"w\", newline='') as file:\n",
        "            writer=csv.writer(file)\n",
        "            writer.writerow([\"review_id\", \"user_id\", \"business_id\", \"stars\", \"useful\", \"funny\", \"cool\", \"text\", \"date\"])\n",
        "            #Loop through the restaurant review file and write to our output file\n",
        "            with open('/content/drive/MyDrive/business_review.json', 'r') as f:\n",
        "                for line in f:\n",
        "                    dict=json.loads(line)\n",
        "                    if(dict[\"business_id\"] in list_of_unique_business_id):\n",
        "                        review_id=dict[\"review_id\"]\n",
        "                        user_id=dict[\"user_id\"]\n",
        "                        business_id=dict[\"business_id\"]\n",
        "                        stars=dict[\"stars\"]\n",
        "                        useful=dict[\"useful\"]\n",
        "                        funny=dict[\"funny\"]\n",
        "                        cool=dict[\"cool\"]\n",
        "                        text=dict[\"text\"]\n",
        "                        date=dict[\"date\"]\n",
        "                        writer.writerow([review_id, user_id, business_id, stars, useful, funny, cool, text, date])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "zsgGws49A4vK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "\"\"\"\n",
        "   Taken from  https://github.com/D3Mlab/rir/blob/main/prefernce_matching/LM.py\n",
        "\"\"\"\n",
        "\n",
        "def create_model(BERT_name, from_pt=True):\n",
        "    ## BERT encoder\n",
        "    encoder = TFAutoModel.from_pretrained(BERT_name, from_pt=True)\n",
        "\n",
        "    ## Model\n",
        "    input_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    # token_type_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    embedding = encoder(\n",
        "        # input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        input_ids=input_ids, attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    model = keras.Model(\n",
        "        # inputs=[input_ids, attention_mask, token_type_ids],\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=embedding, )\n",
        "\n",
        "    model.compile()\n",
        "    return model, input_ids.name, attention_mask.name\n",
        "\n",
        "class BERT_model:\n",
        "    def __init__(self, BERT_name, tokenizer_name, from_pt=False):\n",
        "        \"\"\"\n",
        "        :param BERT_name: name or address of language prefernce_matching\n",
        "        :param tokenizer_name: name or address of the tokenizer\n",
        "        \"\"\"\n",
        "        self.BERT_name = BERT_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.bert_model, self.name1, self.name2 = create_model(BERT_name, from_pt)\n",
        "           \n",
        "    def embed(self, texts, strategy=None, bs=48, verbose=0):\n",
        "        tokenized_review = self.tokenizer.batch_encode_plus(\n",
        "            texts,\n",
        "            max_length=512,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            # truncation_strategy='longest_first',\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        data = {self.name1: tokenized_review['input_ids'],\n",
        "                self.name2: tokenized_review['attention_mask'],\n",
        "                # 'input_3': tokenized_review['token_type_ids']\n",
        "                }\n",
        "        \n",
        "        if strategy is not None:\n",
        "            with strategy.scope():\n",
        "                dataset = tf.data.Dataset.from_tensor_slices(data).batch(bs, drop_remainder=False).prefetch(\n",
        "                    buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "                outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "                return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n",
        "        else:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(data).prefetch(\n",
        "                buffer_size=tf.data.experimental.AUTOTUNE).batch(bs, drop_remainder=False)\n",
        "            outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "            return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Nb1dvidpArXs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class EmbedderCreator():\n",
        "    def __init__(self, model:BERT_model):\n",
        "        #This model is the model used to convert the restaurant review file into\n",
        "        #embeddings\n",
        "        self.embedding_model=model\n",
        "\n",
        "    def embed(self, review_file_path:str, embedding_file_path:str):\n",
        "        df = \"\"\n",
        "        index = 0\n",
        "        if(os.path.exists(embedding_file_path)):\n",
        "            df = pd.read_csv(embedding_file_path)\n",
        "            index = df.shape[0]\n",
        "        else:\n",
        "            # Creating the column title\n",
        "            df = pd.DataFrame({\n",
        "            'Review': [],\n",
        "            'Embedding': [],\n",
        "            \"Business_ID\":[]\n",
        "            })\n",
        "\n",
        "            # Writing the DataFrame to CSV\n",
        "            df.to_csv(embedding_file_path, index=False)\n",
        "\n",
        "        review_dataset=pd.read_csv(review_file_path)\n",
        "\n",
        "        size=len(review_dataset[\"text\"])\n",
        "\n",
        "        # Writing the embedding into the file\n",
        "        df = pd.read_csv(embedding_file_path)\n",
        "\n",
        "        for i in range(index, size):\n",
        "            review=review_dataset[\"text\"][i]\n",
        "            embedding=self.embedding_model.embed([review])\n",
        "            embedding=torch.tensor(embedding)\n",
        "            embedding=embedding.squeeze(0)\n",
        "            embedding=embedding.tolist()\n",
        "\n",
        "            # Defining new data as a dictionary\n",
        "            new_data = {'Review': review_dataset[\"text\"][i], 'Embedding': str(embedding), \"Business_ID\":review_dataset[\"business_id\"][i]}\n",
        "\n",
        "            # Transforming it into a DataFrame\n",
        "            new_data_df = pd.DataFrame(new_data, index=[0])\n",
        "\n",
        "            # Appending the new data\n",
        "            df = pd.concat([df, new_data_df], ignore_index=False)\n",
        "\n",
        "            if(i%100==0):\n",
        "                print(i/100)\n",
        "                df.to_csv(embedding_file_path, index=False)\n",
        "\n",
        "        # Writing the updated data back to CSV\n",
        "        df.to_csv(embedding_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "S94ayKeNceQG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class SortMetaData():\n",
        "    def sort_meta_data(self, source_file, destination_file):\n",
        "        # load your data\n",
        "        df = pd.read_csv(source_file)\n",
        "\n",
        "        # sort the dataframe by the column of interest\n",
        "        df = df.sort_values(by='business_id')\n",
        "\n",
        "        # save your data back to csv\n",
        "        df.to_csv(destination_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "HUrYxiKYAzru"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class SortEmbedding():\n",
        "    def sort_embedding(self, source_file, destination_file):\n",
        "        # load your data\n",
        "        df = pd.read_csv(source_file)\n",
        "\n",
        "        # sort the dataframe by the column of interest\n",
        "        df = df.sort_values(by='Business_ID')\n",
        "\n",
        "        # save your data back to csv\n",
        "        df.to_csv(destination_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "v-DukD2PTgM9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import ast\n",
        "\n",
        "class CreateMatrix():\n",
        "    def create_matrix(self, source_file, destination_file):\n",
        "        # Loop through the sorted embedding csv file\n",
        "        df=pd.read_csv(source_file)\n",
        "\n",
        "        container=[]\n",
        "\n",
        "        size=len(df[\"Embedding\"])\n",
        "\n",
        "        for i in range(size):\n",
        "            embedding=ast.literal_eval(df[\"Embedding\"][i])\n",
        "            embedding=torch.tensor(embedding)\n",
        "            container.append(embedding)\n",
        "\n",
        "        container=torch.stack(container)\n",
        "\n",
        "        torch.save(container, destination_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "QZMm4WX1EYi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class CreateItemSeperation():\n",
        "    def get_item_seperation(self, source_file_path:str, destination_file_path:str):\n",
        "        # Load your CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(source_file_path)\n",
        "\n",
        "        # Group by the specified column and count the number of rows in each group\n",
        "        value_counts = df.groupby('Business_ID').size()\n",
        "\n",
        "        #Convert it into a list\n",
        "        value_counts = value_counts.to_list()\n",
        "\n",
        "        #Change it into a tensor\n",
        "        tensor = torch.tensor(value_counts)\n",
        "\n",
        "        torch.save(tensor, destination_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "zc3qTU9oA2rp"
      },
      "outputs": [],
      "source": [
        "class PreprocessData():\n",
        "    def __init__(self, model_name:str):\n",
        "        self.model_name=model_name\n",
        "        self.embedding_model=BERT_model(self.model_name, self.model_name)\n",
        "        self.top_restaurants_retriever=RetrieveTopRestaurant()\n",
        "        self.convert_to_CSV=Json_to_CSV()\n",
        "        self.clean = Remove_Weird_Stuff()\n",
        "        self.find_review=FilterReview()\n",
        "        self.create_embedding=EmbedderCreator(self.embedding_model)\n",
        "        self.sort_meta_data=SortMetaData()\n",
        "        self.sort_embedding=SortEmbedding()\n",
        "        self.matrix=CreateMatrix()\n",
        "        self.item=CreateItemSeperation()\n",
        "\n",
        "    def preprocess_data(self, source_restaurant_info:str, source_restaurant_review:str):\n",
        "        size = \"all\"\n",
        "        size_str = str(size)\n",
        "        if(size == \"all\"):\n",
        "            size = -1\n",
        "        \n",
        "        #Filter out all the restaurants in Edmonton that are open and has opening hours\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants.json\"):\n",
        "            self.top_restaurants_retriever.get_top_restaurant(source_restaurant_info, \"top_\"+size_str+\"_restaurants.json\", size)\n",
        "        \n",
        "        #Change the json file into a csv file\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants.csv\"):\n",
        "            self.convert_to_CSV.get_csv(\"top_\"+size_str+\"_restaurants.json\", \"top_\"+size_str+\"_restaurants.csv\")\n",
        "\n",
        "        #Clean up the csv file\n",
        "        if os.path.isfile(\"top_\"+size_str+\"_restaurants.csv\"):\n",
        "            self.clean.clean(\"top_\"+size_str+\"_restaurants.csv\")\n",
        "        \n",
        "        #Find all the reviews for all the filtered restaurants\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_review.csv\"):\n",
        "            self.find_review.filter_review(\"top_\"+size_str+\"_restaurants.json\", \"top_\"+size_str+\"_restaurants_review.csv\")\n",
        "\n",
        "        #Sort the meta data for all filtered restaurants\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_sorted.csv\"):\n",
        "            self.sort_meta_data.sort_meta_data(\"top_\"+size_str+\"_restaurants.csv\", \"top_\"+size_str+\"_restaurants_sorted.csv\")\n",
        "        \n",
        "        #Embed all the reviews\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_review_embedding.csv\"):\n",
        "            self.create_embedding.embed(\"top_\"+size_str+\"_restaurants_review.csv\", \"top_\"+size_str+\"_restaurants_review_embedding.csv\")\n",
        "\n",
        "        #Sort all the embedding\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_review_embedding_sorted.csv\"):\n",
        "            self.sort_embedding.sort_embedding(\"top_\"+size_str+\"_restaurants_review_embedding.csv\", \"top_\"+size_str+\"_restaurants_review_embedding_sorted.csv\")\n",
        "\n",
        "        #Get the matrix and item tensor\n",
        "        if not os.path.isfile(\"matrix.pt\"+size_str):\n",
        "            self.matrix.create_matrix(\"top_\"+size_str+\"_restaurants_review_embedding_sorted.csv\", \"matrix.pt\"+size_str)\n",
        "\n",
        "        if not os.path.isfile(\"item.pt\"+size_str):\n",
        "            self.item.get_item_seperation(\"top_\"+size_str+\"_restaurants_review_embedding_sorted.csv\", \"item.pt\"+size_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "MF0UvULDBTNs",
        "outputId": "f78e3d02-d30b-4cba-e530-353e9313ff80"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-122-e63fe9203f2f>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"attributes\"][i] = str(dic)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Music\n",
            "0.0\n",
            "1.0\n",
            "2.0\n",
            "3.0\n",
            "4.0\n",
            "5.0\n",
            "6.0\n",
            "7.0\n",
            "8.0\n",
            "9.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-2ea044f080f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/business_info.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/business_review.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-130-300ce2750e91>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(self, source_restaurant_info, source_restaurant_review)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#Embed all the reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_restaurants_review_embedding.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_restaurants_review.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"top_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_restaurants_review_embedding.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Sort all the embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-d24de99f2b49>\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, review_file_path, embedding_file_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mreview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreview_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-c34e2f85233f>\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, texts, strategy, bs, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m             dataset = tf.data.Dataset.from_tensor_slices(data).prefetch(\n\u001b[1;32m     68\u001b[0m                 buffer_size=tf.data.experimental.AUTOTUNE).batch(bs, drop_remainder=False)\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2380\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_preprocessing=PreprocessData(\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\")\n",
        "    data_preprocessing.preprocess_data(\"/content/drive/MyDrive/business_info.json\", \"/content/drive/MyDrive/business_review.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}