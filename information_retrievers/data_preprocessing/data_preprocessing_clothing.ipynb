{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZPmDPwtZ_6b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install faiss-cpu==1.7.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0jdzvRjJGG0"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import gzip\n",
        "import pandas as pd\n",
        "\n",
        "# List of color and material\n",
        "list_of_color = ['blue', 'red', 'green', 'purple', 'black', 'orange', 'yellow', 'gold', 'white', 'silver', 'pink', 'turquoise', 'sky blue', 'sapphire blue',\\\n",
        "                 'aquamarine', 'cyan', 'blood red', 'lime green', 'gray', 'dark blue', 'teal', 'violet', 'brown', 'emerald green', 'light blue', 'lavender', 'baby blue', 'mint green',\\\n",
        "                 'ruby red', 'indigo', 'navy blue', 'aqua', 'royal blue', 'chrome', 'amethyst', 'neon green', 'scarlet', 'cobalt blue', 'sunset orange', 'azure', 'electric blue', 'neon red',\\\n",
        "                 'light pink', 'hot pink', 'bright yellow', 'coral', 'platinum', 'midnight purple', 'grass green', 'sea green']\n",
        "\n",
        "list_of_material = ['cotton', 'wool', 'silk', 'linen', 'polyester', 'nylon', 'acrylic', 'spandex']\n",
        "\n",
        "def save_top_k_data(input_filepath, output_filepath, k):\n",
        "    \"\"\"Save the top items\n",
        "\n",
        "    Args:\n",
        "        input_filepath (str): The file path towards the compressed file that stores metadata.\n",
        "        output_filepath (str): The output file path towards the compressed file that stores metadata of the top k items.\n",
        "        k (int): Number of items to be saved.\n",
        "\n",
        "    Returns:\n",
        "        None: Stores the metadata of the top k items in the output file path.\n",
        "    \"\"\"\n",
        "    if(os.path.exists(output_filepath)):\n",
        "        return None\n",
        "    g = gzip.open(input_filepath, 'rb')\n",
        "    with gzip.open(output_filepath, 'ab') as f:\n",
        "        for l in g:\n",
        "            if k == 0:\n",
        "                break\n",
        "            f.write(l)\n",
        "            k -= 1\n",
        "\n",
        "\n",
        "def check_for_color(item):\n",
        "    \"\"\"Check for color in the metadata\n",
        "\n",
        "    Args:\n",
        "        item (dict): The dictionary that stores the metadata of an item.\n",
        "\n",
        "    Returns:\n",
        "        dict: The dictionary that stores the metadata of an item with the color added.\n",
        "    \"\"\"\n",
        "    color_found = set()\n",
        "    if(\"category\" in item):\n",
        "        for i in range(len(list_of_color)):\n",
        "            for category in item[\"category\"]:\n",
        "                if(list_of_color[i] in category.lower()):\n",
        "                    color_found.add(list_of_color[i])\n",
        "\n",
        "    if(\"feature\" in item):\n",
        "        for i in range(len(list_of_color)):\n",
        "            for feature in item[\"feature\"]:\n",
        "                if(list_of_color[i] in feature.lower()):\n",
        "                    color_found.add(list_of_color[i])\n",
        "\n",
        "    if(\"description\" in item):\n",
        "        for i in range(len(list_of_color)):\n",
        "            for description in item[\"description\"]:\n",
        "                if(list_of_color[i] in description.lower()):\n",
        "                    color_found.add(list_of_color[i])\n",
        "\n",
        "    if(\"details\" in item):\n",
        "        for i in range(len(list_of_color)):\n",
        "            if(list_of_color[i] in item[\"details\"].lower()):\n",
        "                color_found.add(list_of_color[i])\n",
        "\n",
        "    color_found = list(color_found)\n",
        "    if(len(color_found)>0):\n",
        "        item['color'] = color_found\n",
        "\n",
        "    return item\n",
        "\n",
        "def check_for_material(item):\n",
        "    \"\"\"Checks for material in the metadata and store it into optional if found.\n",
        "\n",
        "    Args:\n",
        "        item (dict): Dictionary that stores the metadata of an item.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary that stores the metadata of an item with the material added.\n",
        "    \"\"\"\n",
        "    material_found = set()\n",
        "    if(\"category\" in item):\n",
        "        for i in range(len(list_of_material)):\n",
        "            for category in item[\"category\"]:\n",
        "                if(list_of_material[i] in category.lower()):\n",
        "                    material_found.add(list_of_material[i])\n",
        "\n",
        "    if(\"feature\" in item):\n",
        "        for i in range(len(list_of_material)):\n",
        "            for feature in item[\"feature\"]:\n",
        "                if(list_of_material[i] in feature.lower()):\n",
        "                    material_found.add(list_of_material[i])\n",
        "\n",
        "    if(\"description\" in item):\n",
        "        for i in range(len(list_of_material)):\n",
        "            for description in item[\"description\"]:\n",
        "                if(list_of_material[i] in description.lower()):\n",
        "                    material_found.add(list_of_material[i])\n",
        "\n",
        "    if(\"details\" in item):\n",
        "        for i in range(len(list_of_material)):\n",
        "            if(list_of_material[i] in item[\"details\"].lower()):\n",
        "                material_found.add(list_of_material[i])\n",
        "\n",
        "    material_found = list(material_found)\n",
        "    if(len(material_found)>0):\n",
        "        item['material'] = material_found\n",
        "\n",
        "    return item\n",
        "\n",
        "def move_fit(item):\n",
        "    \"\"\"Move the fit data to the category, and store it as a sentence\n",
        "\n",
        "    Args:\n",
        "        item (dict): Dictionary that stores the metadata of an item.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary that stores the metadata of an item with the fit data moved to the category.\n",
        "    \"\"\"\n",
        "    too_small = 0\n",
        "    small = 0\n",
        "    perfect = 0\n",
        "    big = 0\n",
        "    too_big = 0\n",
        "\n",
        "    if('fit' in item):\n",
        "        if('Too small' in item['fit']):\n",
        "            # This data is very unorganized\n",
        "            too_small = item['fit']['Too small']\n",
        "            small = item['fit']['Somewhat small']\n",
        "            perfect = item['fit']['Fits as expected']\n",
        "            big = item['fit']['Somewhat large']\n",
        "            too_big = item['fit']['Too large']\n",
        "            item.pop(\"fit\")\n",
        "\n",
        "            sentence = f\"{too_small} people thinks it is too small, {small} people thinks it is somewhat small, \\\n",
        "            {perfect} people thinks it is fits as expected, {big} people thinks it is somewhat large and {too_big} people thinks it is too large.\"\n",
        "\n",
        "            item['category'].append(sentence)\n",
        "\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVfY9blmF63L"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "keywords = ['deliver(y|ies|ed|ing)?', 'ship(ping|ped)?', 'service(s)?', 'packag(e|ing|ed|s)?', 'mail(s|ing)?', 'ship(s)?', 'arrive(s|d)?',\n",
        "            'road', 'Amazon Your review could not be posted', 'UPS', 'FedEx', 'shipment', 'packaging', 'helpful', 'Responsive','Return',\n",
        "            'seller', 'refund', 'communication', 'refund', 'communication', 'resolved', 'follow up', 'courteous(ness|ize|ing)?', 'replace',\n",
        "            'exchange', 'support', 'delay', 'tracking', 'box', 'receiv(ed|es|ing)', 'week(s)','day(s)']\n",
        "\n",
        "def parse(path: str):\n",
        "    \"\"\"Go through the compressed file and yield the data\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path towards the compressed file.\n",
        "\n",
        "    Yields:\n",
        "        dict: A dictionary that stores the metadata of an item.\n",
        "    \"\"\"\n",
        "    g = gzip.open(path, 'rb')\n",
        "    for l in g:\n",
        "         yield json.loads(l)\n",
        "\n",
        "\n",
        "def html_table_to_json(html: str) -> dict:\n",
        "    \"\"\"This function converts the html table to a json dictionary\n",
        "\n",
        "    Args:\n",
        "        html (str): A string that stores the html table.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary that stores the html table.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    if soup is None:\n",
        "        return {}\n",
        "\n",
        "    # Find all rows in the table\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    # Create an empty dictionary to store the data\n",
        "    data_dict = {}\n",
        "\n",
        "    # Iterate over each row\n",
        "    for row in rows:\n",
        "        # Extract the key (rating) from the first column\n",
        "        key = row.find('span').text.strip()\n",
        "\n",
        "        # Extract the value (count) from the last column\n",
        "        value = int(row.find('span').find_next('span').text.strip().replace(',', ''))\n",
        "\n",
        "        # Add the key-value pair to the dictionary\n",
        "        data_dict[key] = value\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "def is_html(text: str) -> bool:\n",
        "    \"\"\"This function checks if the text is html\n",
        "\n",
        "    Args:\n",
        "        text (str): A string that stores the text.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the text is html, False otherwise.\n",
        "    \"\"\"\n",
        "    return bool(re.search(r'</\\w.*?>', text))\n",
        "\n",
        "\n",
        "def is_javascript(text: str) -> bool:\n",
        "    \"\"\"This function checks if the text is javascript\n",
        "\n",
        "    Args:\n",
        "        text (str): A string that stores the text.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the text is javascript, False otherwise.\n",
        "    \"\"\"\n",
        "    return bool(re.search(r\"\\bvar\\b\", text))\n",
        "\n",
        "\n",
        "def remove_unnecessary_space(text: str) -> str:\n",
        "    \"\"\"This function removes unnecessary space in the text\n",
        "\n",
        "    Args:\n",
        "        text (str): A string that stores the text.\n",
        "\n",
        "    Returns:\n",
        "        str: A string that stores the text without unnecessary space.\n",
        "    \"\"\"\n",
        "    return re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "\n",
        "def convert_html_to_readable_text(html: str) -> str:\n",
        "    \"\"\"This function converts html to readable text\n",
        "\n",
        "    Args:\n",
        "        html (str): A string that stores the html.\n",
        "\n",
        "    Returns:\n",
        "        str: A string that stores the readable text.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    if soup is None:\n",
        "        return html\n",
        "    return soup.get_text()\n",
        "\n",
        "\n",
        "def is_review_relevant(review: dict) -> bool:\n",
        "    \"\"\"This function checks if the review is relevant(It must not contain certain keywords)\n",
        "\n",
        "    Args:\n",
        "        review (dict): A dictionary that stores the information of a review.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the review is relevant, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    regex_pattern = r'\\b(' + '|'.join(keywords) + r')\\b'\n",
        "\n",
        "    return review.get('summary', '').lower() not in {'delivery', 'service'} and \\\n",
        "                len(review['reviewText'].split(\" \")) > 30 and \\\n",
        "                not re.search(regex_pattern, review[\"reviewText\"], re.IGNORECASE) and \\\n",
        "                not is_html(review[\"reviewText\"])\n",
        "\n",
        "\n",
        "def is_valid(item: dict) -> bool:\n",
        "    \"\"\"This function checks if the item is valid\n",
        "\n",
        "    Args:\n",
        "        item (dict): A dictionary that stores the metadata of an item.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the item is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    return all('Clothing' in item['category'] and key in item and len(item[key]) != 0 for key in {'price', 'asin', 'feature', 'category', 'title', 'rank', 'brand'}) \\\n",
        "           and not is_html(item['title']) and not is_javascript(item['title'])\n",
        "\n",
        "\n",
        "def preprocess_data(input_metadata_filepath: str, input_reviews_filepath: str, output_metadata_filepath: str, output_reviews_filepath: str):\n",
        "    \"\"\"Preprocess the metadata.\n",
        "    \n",
        "    Remove all items where:\n",
        "        - \"price\" field are empty\n",
        "        - \"feature\" field are empty\n",
        "        - \"category\" field are empty\n",
        "        - \"title\" field are javascript or html\n",
        "\n",
        "    Args:\n",
        "        input_metadata_filepath (str): The file path towards the compressed file that stores metadata.\n",
        "        input_reviews_filepath (str): The file path towards the compressed file that stores reviews.\n",
        "        output_metadata_filepath (str): The output file path towards the csv file that stores metadata.\n",
        "        output_reviews_filepath (str): The output file path towards the csv file that stores reviews.\n",
        "    \n",
        "    Returns:\n",
        "        None: The preprocessed data is stored in the output file path.\n",
        "    \"\"\"\n",
        "    num_valid_item = 0\n",
        "    num_total_item = 0\n",
        "    num_valid_review = 0\n",
        "    num_total_review = 0\n",
        "    items = {}\n",
        "    set_of_optional_keys = set()\n",
        "\n",
        "    if os.path.exists(output_metadata_filepath) or os.path.exists(output_reviews_filepath):\n",
        "        print('output file already exists')\n",
        "        return\n",
        "\n",
        "    for item in parse(input_metadata_filepath):\n",
        "\n",
        "        # check if item is valid.\n",
        "        if is_valid(item):\n",
        "            # convert \"fit\" field to json from html table\n",
        "            if 'fit' in item:\n",
        "                dict_fit = html_table_to_json(f\"<table{item['fit']} </table>\")\n",
        "                item['fit'] = dict_fit\n",
        "\n",
        "            # convert \"details\" field to readable text from html\n",
        "            if 'details' in item:\n",
        "                item['details'] = convert_html_to_readable_text(item['details'])\n",
        "\n",
        "            # remove empty string from categories\n",
        "            new_categories = []\n",
        "            for category in item['category']:\n",
        "                if category != \"\":\n",
        "                    if is_html(category):\n",
        "                        text = convert_html_to_readable_text(category)\n",
        "                    else:\n",
        "                        text = category\n",
        "                    new_categories.append(remove_unnecessary_space(text))\n",
        "            item['category'] = new_categories\n",
        "\n",
        "            # remove empty string from features\n",
        "            new_features = []\n",
        "            for feature in item['feature']:\n",
        "                if feature != \"\":\n",
        "                    if is_html(feature):\n",
        "                        text = convert_html_to_readable_text(feature)\n",
        "                    else:\n",
        "                        text = feature\n",
        "                    new_features.append(remove_unnecessary_space(text))\n",
        "            item['feature'] = new_features\n",
        "\n",
        "            if item['asin'] not in items:\n",
        "                items[item['asin']] = item\n",
        "\n",
        "        num_total_item += 1\n",
        "\n",
        "    print(\"Finished Metadata clean up\")\n",
        "    for review in parse(input_reviews_filepath):\n",
        "        asin = review['asin']\n",
        "        if asin in items:\n",
        "            if 'num_ratings' not in items[asin]:\n",
        "                items[asin]['num_ratings'] = 0\n",
        "            if 'total_stars' not in items[asin]:\n",
        "                items[asin]['total_stars'] = 0\n",
        "\n",
        "            items[asin]['num_ratings'] += 1\n",
        "            items[asin]['total_stars'] += float(review['overall'])\n",
        "\n",
        "            if 'reviewText' in review and is_review_relevant(review):\n",
        "                if 'reviews' not in items[asin]:\n",
        "                    items[asin]['reviews'] = []\n",
        "\n",
        "                items[asin]['reviews'].append([asin, review['reviewText'], review['overall']])\n",
        "                num_valid_review += 1\n",
        "            num_total_review += 1\n",
        "\n",
        "    print(\"Finished Reviews clean up\")\n",
        "    with open(output_metadata_filepath, mode='w', newline='', encoding='utf-8') as meta_file:\n",
        "        with open(output_reviews_filepath, mode='w', newline='', encoding='utf-8') as review_file:\n",
        "            meta_writer = csv.writer(meta_file)\n",
        "            review_writer = csv.writer(review_file)\n",
        "\n",
        "            meta_writer.writerow(['item_id', 'name', 'category', 'price', 'brand', 'rating', 'num_reviews', 'rank', 'imageURLs', 'optional'])\n",
        "            review_writer.writerow(['item_id', 'text', 'stars'])\n",
        "            for asin, item in items.items():\n",
        "                if 'reviews' in item and len(item['reviews']) > 3:\n",
        "                    item = check_for_color(item)\n",
        "                    item = check_for_material(item)\n",
        "                    item = move_fit(item)\n",
        "                    rating = round(item['total_stars'] / item['num_ratings'], 2)\n",
        "                    optional = {\n",
        "                        key: item[key] for key in item if\n",
        "                        key not in {'asin', 'title', 'category', 'price', 'brand', 'feature', 'rating', 'num_reviews',\n",
        "                                    'rank', 'imageURLHighRes', 'main_cat', 'date', 'also_buy', 'also_view', 'imageURL',\n",
        "                                    'total_stars', 'num_ratings', 'reviews'}\n",
        "                    }\n",
        "                    for key in item.keys():\n",
        "                        if(key not in {'asin', 'title', 'category', 'price', 'brand', 'feature', 'rating', 'num_reviews',\n",
        "                                    'rank', 'imageURLHighRes', 'main_cat', 'date', 'also_buy', 'also_view', 'imageURL',\n",
        "                                    'total_stars', 'num_ratings', 'reviews'}):\n",
        "                            set_of_optional_keys.add(key)\n",
        "\n",
        "                    if('details' in optional):\n",
        "                        optional['details'] = optional['details'].replace('\\n\\n', '')\n",
        "                        optional['details'] = optional['details'].replace('\\n', ' ')\n",
        "                    meta_writer.writerow([item['asin'], item['title'], list(set(item['category'] + item['feature'])), item['price'], item['brand'],\n",
        "                                         rating, len(item['reviews']), item['rank'],\n",
        "                                          item.get('imageURLHighRes', []), optional])\n",
        "                    review_writer.writerows(item['reviews'])\n",
        "                    num_valid_item += 1\n",
        "\n",
        "    print(\"Finished Saving\")\n",
        "\n",
        "    print(f'Valid reviews: {num_valid_review}/{num_total_review} ({round(num_valid_review / num_total_review, 4) * 100}%)')\n",
        "    print(f'Valid items: {num_valid_item}/{num_total_item} ({round(num_valid_item / num_total_item, 4) * 100}%)')\n",
        "\n",
        "    return items, set_of_optional_keys\n",
        "\n",
        "\n",
        "num_review = -1\n",
        "save_top_k_data('meta_Clothing_Shoes_and_Jewelry.json.gz',\n",
        "                'top_all_meta_Clothing_Shoes_and_Jewelry.json.gz', num_review)\n",
        "\n",
        "items, set_of_optional_keys = preprocess_data('all-preprocessed-data/top_all_meta_Clothing_Shoes_and_Jewelry.json.gz',\n",
        "                'Clothing_Shoes_and_Jewelry.json.gz',\n",
        "                'processed_top_all_meta_Clothing_Shoes_and_Jewelry.csv',\n",
        "                'processed_top_all_review_Clothing_Shoes_and_Jewelry.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BKwOi_SXNmE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "\"\"\"\n",
        "   Taken from  https://github.com/D3Mlab/rir/blob/main/prefernce_matching/LM.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def create_model(BERT_name, from_pt=True):\n",
        "    ## BERT encoder\n",
        "    encoder = TFAutoModel.from_pretrained(BERT_name, from_pt=True)\n",
        "\n",
        "    ## Model\n",
        "    input_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    # token_type_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    embedding = encoder(\n",
        "        # input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        input_ids=input_ids, attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    model = keras.Model(\n",
        "        # inputs=[input_ids, attention_mask, token_type_ids],\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=embedding, )\n",
        "\n",
        "    model.compile()\n",
        "    return model, input_ids.name, attention_mask.name\n",
        "\n",
        "\n",
        "class BERT_model:\n",
        "    def __init__(self, BERT_name, tokenizer_name, from_pt=False):\n",
        "        \"\"\"\n",
        "        :param BERT_name: name or address of language prefernce_matching\n",
        "        :param tokenizer_name: name or address of the tokenizer\n",
        "        \"\"\"\n",
        "        self.BERT_name = BERT_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.bert_model, self.name1, self.name2 = create_model(BERT_name, from_pt)\n",
        "\n",
        "    def embed(self, texts, strategy=None, bs=48, verbose=0):\n",
        "        tokenized_review = self.tokenizer.batch_encode_plus(\n",
        "            texts,\n",
        "            max_length=512,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            # truncation_strategy='longest_first',\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        data = {self.name1: tokenized_review['input_ids'],\n",
        "                self.name2: tokenized_review['attention_mask'],\n",
        "                # 'input_3': tokenized_review['token_type_ids']\n",
        "                }\n",
        "\n",
        "        if strategy is not None:\n",
        "            with strategy.scope():\n",
        "                dataset = tf.data.Dataset.from_tensor_slices(data).batch(bs, drop_remainder=False).prefetch(\n",
        "                    buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "                outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "                return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n",
        "        else:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(data).prefetch(\n",
        "                buffer_size=tf.data.experimental.AUTOTUNE).batch(bs, drop_remainder=False)\n",
        "            outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "            return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm_hmJdwXA9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class EmbedderCreator:\n",
        "    def __init__(self, model: BERT_model):\n",
        "        # This model is the model used to convert the restaurant review file into\n",
        "        # embeddings\n",
        "        self.embedding_model = model\n",
        "\n",
        "    def embed(self, review_file_path: str, embedding_file_path: str):\n",
        "        \"\"\"Create the embedding for the review file\n",
        "\n",
        "        Args:\n",
        "            review_file_path (str): The file path towards the review file.\n",
        "            embedding_file_path (str): The file path towards the embedding file.\n",
        "            \n",
        "        Returns:\n",
        "            None: The preprocessed data is stored in the output file path.\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        if os.path.exists(embedding_file_path):\n",
        "            df = pd.read_csv(embedding_file_path)\n",
        "            index = df.shape[0]\n",
        "        else:\n",
        "            # Creating the column title\n",
        "            df = pd.DataFrame({\n",
        "                'Review': [],\n",
        "                'Embedding': [],\n",
        "                \"item_id\": []\n",
        "            })\n",
        "\n",
        "            # Writing the DataFrame to CSV\n",
        "            df.to_csv(embedding_file_path, index=False)\n",
        "\n",
        "        review_dataset = pd.read_csv(review_file_path)\n",
        "\n",
        "        size = len(review_dataset[\"text\"])\n",
        "\n",
        "        # # Writing the embedding into the file\n",
        "        # df = pd.read_csv(embedding_file_path)\n",
        "\n",
        "        batch_size = 1024\n",
        "\n",
        "        list_of_review = [0] * batch_size\n",
        "        list_of_business_id = [0] * batch_size\n",
        "\n",
        "        for i in range(index, size):\n",
        "            if i % 100 == 0:\n",
        "                print(i / 100)\n",
        "\n",
        "            if i % batch_size == 0 and i != 0:\n",
        "                embedding = self.embedding_model.embed(list_of_review)\n",
        "                embedding = embedding.tolist()\n",
        "                df_new = pd.DataFrame(\n",
        "                    {'Review': list_of_review, 'Embedding': embedding, \"item_id\": list_of_business_id})\n",
        "\n",
        "                # Append df_new to an existing csv file\n",
        "                df_new.to_csv(embedding_file_path, mode='a', header=False, index=False)\n",
        "\n",
        "                list_of_review[0] = review_dataset[\"text\"][i]\n",
        "                list_of_business_id[0] = review_dataset[\"item_id\"][i]\n",
        "            else:\n",
        "                list_of_review[i % batch_size] = review_dataset[\"text\"][i]\n",
        "                list_of_business_id[i % batch_size] = review_dataset[\"item_id\"][i]\n",
        "\n",
        "        # Embed the remaining reviews\n",
        "        remaining_reviews = size % batch_size\n",
        "        list_of_review = list_of_review[:remaining_reviews]\n",
        "        list_of_business_id = list_of_business_id[:remaining_reviews]\n",
        "\n",
        "        embedding = self.embedding_model.embed(list_of_review)\n",
        "        embedding = embedding.tolist()\n",
        "        df_new = pd.DataFrame({'Review': list_of_review, 'Embedding': embedding, \"item_id\": list_of_business_id})\n",
        "\n",
        "        # Append df_new to an existing csv file\n",
        "        df_new.to_csv(embedding_file_path, mode='a', header=False, index=False)\n",
        "\n",
        "\n",
        "model_name = \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\"\n",
        "embedding_model = BERT_model(model_name, model_name)\n",
        "embedder = EmbedderCreator(embedding_model)\n",
        "embedder.embed('processed_top_all_review_Clothing_Shoes_and_Jewelry.csv',\n",
        "                'embeddings_Clothing_Shoes_and_Jewelry.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMOhnA2Ftvet"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "class CreateDatabase():\n",
        "    def create_faiss_database(self, source_embedding_file_path: str, faiss_destination_file_path: str):\n",
        "        \"\"\"This function creates the faiss database and store it into a file.\n",
        "\n",
        "        Args:\n",
        "            source_embedding_file_path (str): The file path towards the embedding csv file.\n",
        "            faiss_destination_file_path (str): The file path towards the faiss database file.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The dataframe that represents the embedding csv file.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Load the metadata CSV file into a pandas DataFrame\n",
        "        df_embedding = pd.read_csv(source_embedding_file_path)\n",
        "\n",
        "        # Create the vector database\n",
        "        dimension_size = 768\n",
        "        index = faiss.IndexFlatIP(dimension_size)  # Create the index, uses dot product to measure similarity\n",
        "\n",
        "        # For each embedding, store it into database. Set the corresponding metadata, id and review array\n",
        "        for i in range(df_embedding.shape[0]):\n",
        "            embedding = df_embedding[\"Embedding\"][i]\n",
        "            embedding = eval(embedding)\n",
        "            embedding_numpy = np.array(embedding)\n",
        "            embedding_numpy = np.expand_dims(embedding_numpy, axis=0)\n",
        "            index.add(embedding_numpy)\n",
        "\n",
        "        faiss.write_index(index, faiss_destination_file_path)\n",
        "\n",
        "        return df_embedding\n",
        "\n",
        "    def convert_clothing_meta_to_json(self, input_filepath, output_filepath):\n",
        "        \"\"\"This function converts the clothing metadata csv file to json file\n",
        "\n",
        "        Args:\n",
        "            input_filepath (str): The file path towards the clothing metadata csv file.\n",
        "            output_filepath (str): The output file path towards the clothing metadata json file.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(input_filepath)\n",
        "        df['category'] = df['category'].apply(eval)\n",
        "        df['rating'] = df['rating'].apply(float)\n",
        "        df['num_reviews'] = df['num_reviews'].apply(float)\n",
        "        df['optional'] = df['optional'].apply(eval)\n",
        "        df.to_json(output_filepath, orient='records', lines=True)\n",
        "\n",
        "\n",
        "    def convert_restaurant_meta_to_json(self, input_file_path, output_filepath):\n",
        "        \"\"\"This function converts the restaurant metadata csv file to json file\n",
        "\n",
        "        Args:\n",
        "            input_file_path (str): The file path towards the restaurant metadata csv file.\n",
        "            output_filepath (str): The output file path towards the restaurant metadata json file.\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(input_file_path)\n",
        "        df['latitude'] = df['latitude'].apply(float)\n",
        "        df['longitude'] = df['longitude'].apply(float)\n",
        "        df['stars'] = df['stars'].apply(float)\n",
        "        df['review_count'] = df['review_count'].apply(int)\n",
        "        df['is_open'] = df['is_open'].apply(bool)\n",
        "        df['categories'] = df['categories'].apply(lambda x: list(x.split(\",\")))\n",
        "        df['hours'] = df['hours'].apply(ast.literal_eval)\n",
        "        df['optional'] = df['optional'].apply(ast.literal_eval)\n",
        "        df.to_json(output_filepath, orient='records', lines=True)\n",
        "\n",
        "\n",
        "    def create_matrix(self, source_file_pandas: pd.DataFrame, destination_file: str):\n",
        "        \"\"\"This function creates the matrix and store it into a pytorch tensor file.\n",
        "\n",
        "        Args:\n",
        "            source_file_pandas (pd.DataFrame): The file path towards the embedding csv file.\n",
        "            destination_file (str): The file path towards the pytorch tensor file.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Loop through the sorted embedding csv file\n",
        "        df = source_file_pandas\n",
        "\n",
        "        container = []\n",
        "\n",
        "        size = len(df[\"Embedding\"])\n",
        "\n",
        "        for i in range(size):\n",
        "            embedding = eval(df[\"Embedding\"][i])\n",
        "            embedding = torch.tensor(embedding)\n",
        "            container.append(embedding)\n",
        "\n",
        "        container = torch.stack(container)\n",
        "\n",
        "        torch.save(container, destination_file)\n",
        "\n",
        "\n",
        "    def create_review(self, source_file_pandas: pd.DataFrame, destination_file: str):\n",
        "        \"\"\"This function creates a csv file that only contain the review and corresponding item id\n",
        "\n",
        "        Args:\n",
        "            source_file_pandas (pd.DataFrame): The pandas dataframe that stores the embedding csv file.\n",
        "            destination_file (str): The file path towards the review csv file.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Delete the embedding column\n",
        "        df = source_file_pandas\n",
        "\n",
        "        df = df.drop('Embedding', axis=1)\n",
        "\n",
        "        df.to_csv(destination_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyRIP0V9u6qb"
      },
      "outputs": [],
      "source": [
        "create_database = CreateDatabase()\n",
        "\n",
        "# Create FAISS database\n",
        "embedding_df = create_database.create_faiss_database(\"embeddings_Clothing_Shoes_and_Jewelry.csv\", \"database.faiss\")\n",
        "\n",
        "# Create item metadata json\n",
        "create_database.convert_clothing_meta_to_json(\"all-preprocessed-data/processed_top_all_meta_Clothing_Shoes_and_Jewelry.csv\", \"item_metadata.json\")\n",
        "\n",
        "# Create the matrix\n",
        "create_database.create_matrix(embedding_df, \"reviews_embedding_matrix.pt\")\n",
        "\n",
        "# Create the item review\n",
        "create_database.create_review(embedding_df, \"items_reviews.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
