{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing for restaurants\n",
        "\n",
        "#### Output files\n",
        "1. item_metadata.json\n",
        "2. top_all_restaurants_review_sorted.csv"
      ],
      "metadata": {
        "id": "fQpyoAwaI35K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNSpCMnMBALx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install faiss-cpu==1.7.4  # FAISS can only load database from same FAISS version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIdb52Gv_4v2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "class RetrieveTopRestaurant():\n",
        "    def get_top_restaurant(self, source_file_path:str, destination_file_path:str, num_restaurant:int):\n",
        "        # Open the original JSON file and load each line\n",
        "        with open(source_file_path, 'r') as file:\n",
        "            data = [json.loads(line) for line in file]\n",
        "\n",
        "        filtered_data=[]\n",
        "\n",
        "        counter=0\n",
        "\n",
        "        # Filter out lines with the desired tag\n",
        "        for item in data:\n",
        "            if(item['categories']==None):\n",
        "                continue\n",
        "            if(\"Restaurants\" in item['categories'] and item[\"is_open\"]==1 and item['city'] == 'Edmonton' and\n",
        "            not pd.isna(item['attributes']) and not pd.isna(item['hours'])):\n",
        "                counter+=1\n",
        "                try:\n",
        "                    filtered_data.append(item)\n",
        "                except:\n",
        "                    print(filtered_data)\n",
        "\n",
        "                if(counter == num_restaurant):\n",
        "                    break\n",
        "\n",
        "        # Write the filtered lines to a new JSON file\n",
        "        with open(destination_file_path, 'w') as file:\n",
        "            for item in filtered_data:\n",
        "                file.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVSJsw3A_tm0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "class Json_to_CSV():\n",
        "    def get_csv(self, source_file_path:str, destination_file_path:str):\n",
        "        \"\"\"This function takes in a json file as an input(source_file_path) and outputs a csv file(destination_file_path)\n",
        "\n",
        "        Args:\n",
        "            source_file_path:str : input json file path\n",
        "            destination_file_path:str : output csv file path\n",
        "        \"\"\"\n",
        "        with open(destination_file_path, \"w\", newline='') as file:\n",
        "            writer=csv.writer(file)\n",
        "            writer.writerow([\"item_id\", \"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\",\n",
        "                            \"longitude\", \"stars\", \"review_count\", \"is_open\", \"attributes\", \"categories\", \"hours\"])\n",
        "\n",
        "            with open(source_file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    # Load the JSON object from the line\n",
        "                    json_obj = json.loads(line)\n",
        "\n",
        "                    item_id=json_obj[\"business_id\"]\n",
        "                    name=json_obj[\"name\"]\n",
        "                    address=json_obj[\"address\"]\n",
        "                    city=json_obj[\"city\"]\n",
        "                    state=json_obj[\"state\"]\n",
        "                    postal_code=json_obj[\"postal_code\"]\n",
        "                    latitude=json_obj[\"latitude\"]\n",
        "                    longitude=json_obj[\"longitude\"]\n",
        "                    stars=json_obj[\"stars\"]\n",
        "                    review_count=json_obj[\"review_count\"]\n",
        "                    is_open=json_obj[\"is_open\"]\n",
        "                    attributes=json_obj[\"attributes\"]\n",
        "                    categories=json_obj[\"categories\"]\n",
        "                    hours=json_obj[\"hours\"]\n",
        "\n",
        "                    writer.writerow([item_id, name, address, city, state, postal_code, latitude, longitude,\n",
        "                                    stars, review_count, is_open, attributes, categories, hours])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwgaXY1chHHh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class Remove_Weird_Stuff():\n",
        "    def clean(self, source_file_path:str):\n",
        "        \"\"\"\n",
        "        Takes in a csv file and change all the attributes values inside the file into normal values\n",
        "\n",
        "        :param source_file_path: file path to the file\n",
        "        :return: A file without the weird u'\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(source_file_path)\n",
        "\n",
        "        size = len(df[\"attributes\"])\n",
        "\n",
        "        for i in range(size):\n",
        "            for key, value in eval(df[\"attributes\"][i]).items():\n",
        "                if key == \"RestaurantsReservations\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"HasReservations\"] = dic[\"RestaurantsReservations\"]\n",
        "                    del dic[\"RestaurantsReservations\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsGoodForGroups\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"GoodForGroups\"] = dic[\"RestaurantsGoodForGroups\"]\n",
        "                    del dic[\"RestaurantsGoodForGroups\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"BestNights\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"PopularNights\"] = dic[\"BestNights\"]\n",
        "                    del dic[\"BestNights\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsPriceRange2\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"PriceRange\"] = dic[\"RestaurantsPriceRange2\"]\n",
        "                    del dic[\"RestaurantsPriceRange2\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsTableService\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"HasTableService\"] = dic[\"RestaurantsTableService\"]\n",
        "                    del dic[\"RestaurantsTableService\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsDelivery\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"HasDelivery\"] = dic[\"RestaurantsDelivery\"]\n",
        "                    del dic[\"RestaurantsDelivery\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsAttire\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"Attire\"] = dic[\"RestaurantsAttire\"]\n",
        "                    del dic[\"RestaurantsAttire\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"RestaurantsTakeOut\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"HasTakeOut\"] = dic[\"RestaurantsTakeOut\"]\n",
        "                    del dic[\"RestaurantsTakeOut\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "                elif key == \"ByAppointmentOnly\":\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[\"MustMakeReservation\"] = dic[\"ByAppointmentOnly\"]\n",
        "                    del dic[\"ByAppointmentOnly\"]\n",
        "                    df[\"attributes\"][i] = str(dic)\n",
        "\n",
        "        for i in range(size):\n",
        "            for key, value in eval(df[\"attributes\"][i]).items():\n",
        "                if(value == \"{}\"):\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic.pop(key)\n",
        "                elif(value == \"None\" or value == \"none\"):\n",
        "                    if key == \"Music\":\n",
        "                        dic = eval(df[\"attributes\"][i])\n",
        "                        dic[key] = \"{dj: False, background_music: False, no_music: False, jukebox: False, live: False, video: False, karaoke: False}\"\n",
        "                    elif key == \"Ambience\":\n",
        "                        dic = eval(df[\"attributes\"][i])\n",
        "                        dic[key] = \"{touristy: False, hipster: False, romantic: False, intimate: False, trendy: False, upscale: False, classy: False, casual: False, divey: False}\"\n",
        "                    elif key == \"BestNights\":\n",
        "                        dic = eval(df[\"attributes\"][i])\n",
        "                        dic[key] = \"{monday: False, tuesday: False, friday: False, wednesday: False, thursday: False, sunday: False, saturday: False}\"\n",
        "                    elif key == \"GoodForMeal\":\n",
        "                        dic[key] = \"{dessert: False, latenight: False, lunch: False, dinner: False, brunch: False, breakfast: False}\"\n",
        "                    elif key == \"BusinessParking\":\n",
        "                        dic[key] = \"{garage: False, street: False, validated: False, lot: False, valet: False}\"\n",
        "                    elif key == \"DietaryRestrictions\":\n",
        "                        dic[key] = \"dairy-free: False, gluten-free: False, vegan: False, kosher: False, halal: False, soy-free: False, vegetarian: False}\"\n",
        "                else:\n",
        "                    value = value.replace(\"u'\", \"\")\n",
        "                    value = value.replace(\"'\", \"\")\n",
        "                    value = value.replace(\"\\\"\", \"\")\n",
        "\n",
        "                    dic = eval(df[\"attributes\"][i])\n",
        "                    dic[key] = value\n",
        "\n",
        "                df[\"attributes\"][i] = str(dic)\n",
        "\n",
        "        df.to_csv(source_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hke0oSDlAb-z"
      },
      "outputs": [],
      "source": [
        "class FilterReview():\n",
        "    def filter_review(self, metadata_source_file_path:str, reviews_source_file_path: str, destination_file_path:str):\n",
        "        #Records the list of unique business_id\n",
        "        list_of_unique_business_id=[]\n",
        "\n",
        "        #Loop through the restaurant info file to collect all the business_id\n",
        "        with open(metadata_source_file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                dict=json.loads(line)\n",
        "                list_of_unique_business_id.append(dict[\"business_id\"])\n",
        "\n",
        "        with open(destination_file_path, \"w\", newline='') as file:\n",
        "            writer=csv.writer(file)\n",
        "            writer.writerow([\"review_id\", \"user_id\", \"item_id\", \"stars\", \"useful\", \"funny\", \"cool\", \"text\", \"date\"])\n",
        "            #Loop through the restaurant review file and write to our output file\n",
        "            with open(reviews_source_file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    dict=json.loads(line)\n",
        "                    if(dict[\"business_id\"] in list_of_unique_business_id):\n",
        "                        review_id=dict[\"review_id\"]\n",
        "                        user_id=dict[\"user_id\"]\n",
        "                        item_id=dict[\"business_id\"]\n",
        "                        stars=dict[\"stars\"]\n",
        "                        useful=dict[\"useful\"]\n",
        "                        funny=dict[\"funny\"]\n",
        "                        cool=dict[\"cool\"]\n",
        "                        text=dict[\"text\"]\n",
        "                        date=dict[\"date\"]\n",
        "                        writer.writerow([review_id, user_id, item_id, stars, useful, funny, cool, text, date])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsgGws49A4vK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import transformers\n",
        "# transformers.logging.set_verbosity_error()\n",
        "\n",
        "\"\"\"\n",
        "   Taken from  https://github.com/D3Mlab/rir/blob/main/prefernce_matching/LM.py\n",
        "\"\"\"\n",
        "\n",
        "def create_model(BERT_name, from_pt=True):\n",
        "    ## BERT encoder\n",
        "    encoder = TFAutoModel.from_pretrained(BERT_name, from_pt=True)\n",
        "\n",
        "    ## Model\n",
        "    input_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    # token_type_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    embedding = encoder(\n",
        "        # input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        input_ids=input_ids, attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    model = keras.Model(\n",
        "        # inputs=[input_ids, attention_mask, token_type_ids],\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs=embedding, )\n",
        "\n",
        "    model.compile()\n",
        "    return model, input_ids.name, attention_mask.name\n",
        "\n",
        "class BERT_model:\n",
        "    def __init__(self, BERT_name, tokenizer_name, from_pt=False):\n",
        "        \"\"\"\n",
        "        :param BERT_name: name or address of language prefernce_matching\n",
        "        :param tokenizer_name: name or address of the tokenizer\n",
        "        \"\"\"\n",
        "        self.BERT_name = BERT_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.bert_model, self.name1, self.name2 = create_model(BERT_name, from_pt)\n",
        "\n",
        "    def embed(self, texts, strategy=None, bs=48, verbose=0):\n",
        "        tokenized_review = self.tokenizer.batch_encode_plus(\n",
        "            texts,\n",
        "            max_length=512,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            # truncation_strategy='longest_first',\n",
        "            padding=\"max_length\",\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        data = {self.name1: tokenized_review['input_ids'],\n",
        "                self.name2: tokenized_review['attention_mask'],\n",
        "                # 'input_3': tokenized_review['token_type_ids']\n",
        "                }\n",
        "\n",
        "        if strategy is not None:\n",
        "            with strategy.scope():\n",
        "                dataset = tf.data.Dataset.from_tensor_slices(data).batch(bs, drop_remainder=False).prefetch(\n",
        "                    buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "                outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "                return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n",
        "        else:\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(data).prefetch(\n",
        "                buffer_size=tf.data.experimental.AUTOTUNE).batch(bs, drop_remainder=False)\n",
        "            outputs = self.bert_model.predict(dataset, verbose=verbose)\n",
        "            return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb1dvidpArXs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel, AutoModel\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class EmbedderCreator():\n",
        "    def __init__(self, model:BERT_model):\n",
        "        #This model is the model used to convert the restaurant review file into\n",
        "        #embeddings\n",
        "        self.embedding_model=model\n",
        "\n",
        "    def embed(self, review_file_path:str, embedding_file_path:str):\n",
        "        df = \"\"\n",
        "        index = 0\n",
        "        if(os.path.exists(embedding_file_path)):\n",
        "            df = pd.read_csv(embedding_file_path)\n",
        "            index = df.shape[0]\n",
        "        else:\n",
        "            # Creating the column title\n",
        "            df = pd.DataFrame({\n",
        "            'Review': [],\n",
        "            \"item_id\":[]\n",
        "            })\n",
        "\n",
        "            # Writing the DataFrame to CSV\n",
        "            df.to_csv(embedding_file_path, index=False)\n",
        "\n",
        "        review_dataset=pd.read_csv(review_file_path)\n",
        "\n",
        "        size=len(review_dataset[\"text\"])\n",
        "\n",
        "        # Writing the embedding into the file\n",
        "        df = pd.read_csv(embedding_file_path)\n",
        "\n",
        "        batch_size = 128\n",
        "\n",
        "        list_of_review = [0] * batch_size\n",
        "        list_of_business_id = [0] * batch_size\n",
        "\n",
        "        for i in range(index, size):\n",
        "            if(i%100==0):\n",
        "                print(i/100)\n",
        "\n",
        "            if(i%batch_size == 0 and i != 0):\n",
        "                embedding = self.embedding_model.embed(list_of_review)\n",
        "                embedding = embedding.tolist()\n",
        "                df_new = pd.DataFrame({'Review': list_of_review, \"item_id\":list_of_business_id})\n",
        "\n",
        "                # Append df_new to an existing csv file\n",
        "                df_new.to_csv(embedding_file_path, mode='a', header=False, index = False)\n",
        "\n",
        "                list_of_review[0] = review_dataset[\"text\"][i]\n",
        "                list_of_business_id[0] = review_dataset[\"item_id\"][i]\n",
        "            else:\n",
        "                list_of_review[i%batch_size] = review_dataset[\"text\"][i]\n",
        "                list_of_business_id[i%batch_size] = review_dataset[\"item_id\"][i]\n",
        "\n",
        "        #Embed the remaining reviews\n",
        "        remaining_reviews = size%batch_size\n",
        "        list_of_review = list_of_review[:remaining_reviews]\n",
        "        list_of_business_id = list_of_business_id[:remaining_reviews]\n",
        "\n",
        "        embedding = self.embedding_model.embed(list_of_review)\n",
        "        embedding = embedding.tolist()\n",
        "        df_new = pd.DataFrame({'Review': list_of_review, \"item_id\":list_of_business_id})\n",
        "\n",
        "        # Append df_new to an existing csv file\n",
        "        df_new.to_csv(embedding_file_path, mode='a', header=False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S94ayKeNceQG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class SortMetaData():\n",
        "    def sort_meta_data(self, source_file, destination_file):\n",
        "        # load your data\n",
        "        df = pd.read_csv(source_file)\n",
        "\n",
        "        # sort the dataframe by the column of interest\n",
        "        df = df.sort_values(by='item_id')\n",
        "\n",
        "        # save your data back to csv\n",
        "        df.to_csv(destination_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUrYxiKYAzru"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class SortEmbedding():\n",
        "    def sort_embedding(self, source_file, destination_file):\n",
        "        # load your data\n",
        "        df = pd.read_csv(source_file)\n",
        "\n",
        "        # sort the dataframe by the column of interest\n",
        "        df = df.sort_values(by='item_id')\n",
        "\n",
        "        # save your data back to csv\n",
        "        df.to_csv(destination_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-DukD2PTgM9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class CreateMatrix():\n",
        "    def create_matrix(self, source_file, destination_file):\n",
        "        # Loop through the sorted embedding csv file\n",
        "        df=pd.read_csv(source_file)\n",
        "\n",
        "        container=[]\n",
        "\n",
        "        size=len(df[\"Embedding\"])\n",
        "\n",
        "        for i in range(size):\n",
        "            embedding= eval(df[\"Embedding\"][i])\n",
        "            embedding=torch.tensor(embedding)\n",
        "            container.append(embedding)\n",
        "\n",
        "        container=torch.stack(container)\n",
        "\n",
        "        torch.save(container, destination_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZMm4WX1EYi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class CreateItemSeperation():\n",
        "    def get_item_seperation(self, source_file_path:str, destination_file_path:str):\n",
        "        # Load your CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(source_file_path)\n",
        "\n",
        "        # Group by the specified column and count the number of rows in each group\n",
        "        value_counts = df.groupby('item_id').size()\n",
        "\n",
        "        #Convert it into a list\n",
        "        value_counts = value_counts.to_list()\n",
        "\n",
        "        #Change it into a tensor\n",
        "        tensor = torch.tensor(value_counts)\n",
        "\n",
        "        torch.save(tensor, destination_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zp-7EQhqL0r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "class CreateDatabase():\n",
        "    def create_database(self, source_embedding_file_path: str, faiss_destination_file_path: str):\n",
        "        # Load the metadata CSV file into a pandas DataFrame\n",
        "        df_embedding = pd.read_csv(source_embedding_file_path)\n",
        "\n",
        "        # Create the vector database\n",
        "        dimension_size = 768\n",
        "        index = faiss.IndexFlatIP(dimension_size)  # Create the index, uses dot product to measure similarity\n",
        "\n",
        "        # Store each embedding into database\n",
        "        for i in range(df_embedding.shape[0]):\n",
        "            embedding_str = df_embedding[\"Embedding\"][i]\n",
        "            embedding_list = eval(embedding_str)\n",
        "            embedding_np = np.array(embedding_list)\n",
        "            embedding_np = embedding_np.reshape(1, 768)\n",
        "            index.add(embedding_np)\n",
        "\n",
        "        faiss.write_index(index, faiss_destination_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0eYzPuTsfaG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "class CreateMetadataStorage():\n",
        "    def _format_optional(self, data: dict):\n",
        "        for key in data:\n",
        "            if data[key].lower() == 'true':\n",
        "                data[key] = 'Yes'\n",
        "            elif data[key].lower() == 'false':\n",
        "                data[key] = 'No'\n",
        "\n",
        "        if 'PriceRange' in data:\n",
        "            if data['PriceRange'] == \"1\":\n",
        "                data['PriceRange'] = \"$0-$10\"\n",
        "            elif data['PriceRange']== \"2\":\n",
        "                data['PriceRange'] = \"$11-$30\"\n",
        "            elif data['PriceRange'] == \"3\":\n",
        "                data['PriceRange'] = \"$31-60\"\n",
        "            elif data['PriceRange'] == \"4\":\n",
        "                data['PriceRange'] = \"$60+\"\n",
        "\n",
        "        return data\n",
        "\n",
        "    def create_json(self, metadata_source_file: str, destination_file_path: str):\n",
        "        df = pd.read_csv(metadata_source_file)\n",
        "        df = df.rename(columns={'attributes': 'optional'})\n",
        "\n",
        "        df['latitude'] = df['latitude'].apply(float)\n",
        "        df['longitude'] = df['longitude'].apply(float)\n",
        "        df['stars'] = df['stars'].apply(float)\n",
        "        df['review_count'] = df['review_count'].apply(int)\n",
        "        df['is_open'] = df['is_open'].apply(bool)\n",
        "        df['categories'] = df['categories'].apply(lambda x: list(x.split(\",\")))\n",
        "        df['hours'] = df['hours'].apply(eval)\n",
        "        df['optional'] = df['optional'].apply(eval)\n",
        "        df['optional'] = df['optional'].apply(self._format_optional)\n",
        "        df.to_json(destination_file_path, orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc3qTU9oA2rp"
      },
      "outputs": [],
      "source": [
        "class PreprocessData():\n",
        "    def __init__(self, model_name:str):\n",
        "        self.model_name=model_name\n",
        "        self.embedding_model=BERT_model(self.model_name, self.model_name)\n",
        "        self.top_restaurants_retriever=RetrieveTopRestaurant()\n",
        "        self.convert_to_CSV=Json_to_CSV()\n",
        "        self.clean = Remove_Weird_Stuff()\n",
        "        self.find_review=FilterReview()\n",
        "        self.create_embedding=EmbedderCreator(self.embedding_model)\n",
        "        self.sort_meta_data=SortMetaData()\n",
        "        self.sort_embedding=SortEmbedding()\n",
        "        self.matrix=CreateMatrix()\n",
        "        self.item=CreateItemSeperation()\n",
        "        self.vector_database = CreateDatabase()\n",
        "        self.metadata_database = CreateMetadataStorage()\n",
        "\n",
        "    def preprocess_data(self, source_restaurant_info:str, source_restaurant_review:str):\n",
        "        size = \"all\"\n",
        "        size_str = str(size)\n",
        "        if(size == \"all\"):\n",
        "            size = -1\n",
        "\n",
        "        #Filter out all the restaurants in Edmonton that are open and has opening hours\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants.json\"):\n",
        "            self.top_restaurants_retriever.get_top_restaurant(source_restaurant_info, \"top_\"+size_str+\"_restaurants.json\", size)\n",
        "\n",
        "        #Change the json file into a csv file\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants.csv\"):\n",
        "            self.convert_to_CSV.get_csv(\"top_\"+size_str+\"_restaurants.json\", \"top_\"+size_str+\"_restaurants.csv\")\n",
        "\n",
        "        #Clean up the csv file\n",
        "        if os.path.isfile(\"top_\"+size_str+\"_restaurants.csv\"):\n",
        "            self.clean.clean(\"top_\"+size_str+\"_restaurants.csv\")\n",
        "\n",
        "        #Find all the reviews for all the filtered restaurants\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_review.csv\"):\n",
        "            self.find_review.filter_review(\"top_\"+size_str+\"_restaurants.json\", source_restaurant_review, \"top_\"+size_str+\"_restaurants_review.csv\")\n",
        "\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_review_sorted.csv\"):\n",
        "            self.sort_file_by_item_id(\"top_\"+size_str+\"_restaurants_review.csv\", \"top_\"+size_str+\"_restaurants_review_sorted.csv\")\n",
        "\n",
        "        #Sort the meta data for all filtered restaurants\n",
        "        if not os.path.isfile(\"top_\"+size_str+\"_restaurants_sorted.csv\"):\n",
        "            self.sort_file_by_item_id(\"top_\"+size_str+\"_restaurants.csv\", \"top_\"+size_str+\"_restaurants_sorted.csv\")\n",
        "\n",
        "        # Data preprocessing for metadata storage\n",
        "        if not os.path.isfile(\"item_metadata.json\"):\n",
        "            self.metadata_database.create_json(\"top_\"+size_str+\"_restaurants_sorted.csv\", \"item_metadata.json\")\n",
        "\n",
        "\n",
        "    def sort_file_by_item_id(self, source_file: str, destination_file: str):\n",
        "        df = pd.read_csv(source_file)\n",
        "        df = df.sort_values(by='item_id')\n",
        "        df.to_csv(destination_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF0UvULDBTNs"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_preprocessing=PreprocessData(\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\")\n",
        "    data_preprocessing.preprocess_data(\"< business info data file from yelp academic dataset >\", \"< reviews data file from yelp academic dataset >\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}